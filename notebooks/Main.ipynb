{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e2c14bd",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# ml_lstm: End-to-end CLI workflow (ingestion òÆÒ paper trading)\n",
    "This notebook collects the major project CLIs needed to run the full workflow from data ingestion to paper trading.High-level steps:\n",
    "1. Run the daily data pipeline (ingestion, cleaning, gap handling, feature engineering).\n",
    "2. Train the LSTM model.\n",
    "3. Evaluate the latest best model.\n",
    "4. Generate per-bar predictions CSV.\n",
    "5. Run a backtest using that CSV.\n",
    "6. Plot backtest diagnostics.\n",
    "7. Run simulated paper trading using the same CSV.All `!` shell commands assume this notebook is in `notebooks/` and the project root is the parent directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "608cfebc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using FREQUENCY = 15min TSTEPS = 5\n",
      "PREDICTIONS_CSV = C:\\Users\\Anton\\SRC\\my\\ml_lstm\\backtests\\nvda_15min_predictions.csv\n",
      "TRADES_CSV      = C:\\Users\\Anton\\SRC\\my\\ml_lstm\\backtests\\nvda_15min_trades.csv\n",
      "EQUITY_CSV      = C:\\Users\\Anton\\SRC\\my\\ml_lstm\\backtests\\nvda_15min_equity.csv\n",
      "PRICE_CSV       = C:\\Users\\Anton\\SRC\\my\\ml_lstm\\data\\processed\\nvda_15min.csv\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import src.config as cfg\n",
    "cfg = importlib.reload(cfg)\n",
    "\n",
    "from src.config import (\n",
    "    get_predictions_csv_path,\n",
    "    get_trades_csv_path,\n",
    "    get_equity_csv_path,\n",
    "    get_hourly_data_csv_path,\n",
    ")\n",
    "\n",
    "FREQUENCY = cfg.FREQUENCY\n",
    "TSTEPS = cfg.TSTEPS\n",
    "\n",
    "PREDICTIONS_CSV = get_predictions_csv_path('nvda', FREQUENCY)\n",
    "TRADES_CSV = get_trades_csv_path('nvda', FREQUENCY)\n",
    "EQUITY_CSV = get_equity_csv_path('nvda', FREQUENCY)\n",
    "PRICE_CSV = get_hourly_data_csv_path(FREQUENCY)\n",
    "\n",
    "print('Using FREQUENCY =', FREQUENCY, 'TSTEPS =', TSTEPS)\n",
    "print('PREDICTIONS_CSV =', PREDICTIONS_CSV)\n",
    "print('TRADES_CSV      =', TRADES_CSV)\n",
    "print('EQUITY_CSV      =', EQUITY_CSV)\n",
    "print('PRICE_CSV       =', PRICE_CSV)\n",
    "\n",
    "\n",
    "def run_module(mod, *args):\n",
    "    cmd = [sys.executable, '-m', mod, *map(str, args)]\n",
    "    print('Running:', ' '.join(cmd))\n",
    "    print('in', project_root)\n",
    "    print()\n",
    "    result = subprocess.run(cmd, cwd=project_root, capture_output=True, text=True)\n",
    "    if result.stdout:\n",
    "        print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(result.stderr)\n",
    "    if result.returncode != 0:\n",
    "        print(f'Command failed with exit code {result.returncode}')\n",
    "    return result.returncode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a445d9",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 1. Daily data pipeline \n",
    "(ingestion processed hourly features)Runs `src.daily_data_agent` which orchestrates ingestion, cleaning, gap handling, curated minute snapshot, and resampling + feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27567445",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: C:\\Users\\Anton\\SRC\\my\\ml_lstm\\venv\\Scripts\\python.exe -m src.daily_data_agent in C:\\Users\\Anton\\SRC\\my\\ml_lstm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the daily data pipeline (ingestion -> processed features). For a dry run that skips IB/TWS ingestion, add '--skip-ingestion' to the arguments below. \n",
    "run_module('src.daily_data_agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dca6b5-51f5-4f52-a31f-2ace97a7579c",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 2. Train LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fa93156-5e24-4a2d-a2d6-857e6bfe1bd6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: C:\\Users\\Anton\\SRC\\my\\ml_lstm\\venv\\Scripts\\python.exe -m src.train --frequency 15min --tsteps 5\n",
      "in C:\\Users\\Anton\\SRC\\my\\ml_lstm\n",
      "\n",
      "\n",
      "--- Training model for frequency: 15min, TSTEPS: 5 ---\n",
      "Scaler parameters calculated on training data and saved to C:\\Users\\Anton\\SRC\\my\\ml_lstm\\data\\processed\\scaler_params_15min.json\n",
      "Starting model training for 15min with TSTEPS=5...\n",
      "Epoch 1/20\n",
      "510/510 - 3s - 6ms/step - loss: 0.0061 - mae: 0.0226 - val_loss: 9.5159e-05 - val_mae: 0.0082\n",
      "Epoch 2/20\n",
      "510/510 - 2s - 4ms/step - loss: 7.4527e-05 - mae: 0.0059 - val_loss: 1.7179e-05 - val_mae: 0.0024\n",
      "Epoch 3/20\n",
      "510/510 - 2s - 3ms/step - loss: 5.2986e-05 - mae: 0.0053 - val_loss: 7.2250e-05 - val_mae: 0.0077\n",
      "Epoch 4/20\n",
      "510/510 - 2s - 3ms/step - loss: 3.9362e-05 - mae: 0.0051 - val_loss: 2.6400e-05 - val_mae: 0.0039\n",
      "Epoch 5/20\n",
      "510/510 - 2s - 4ms/step - loss: 3.8773e-05 - mae: 0.0051 - val_loss: 2.5646e-05 - val_mae: 0.0038\n",
      "Epoch 6/20\n",
      "510/510 - 1s - 3ms/step - loss: 3.8808e-05 - mae: 0.0051 - val_loss: 2.5030e-05 - val_mae: 0.0037\n",
      "Epoch 7/20\n",
      "510/510 - 2s - 3ms/step - loss: 3.8775e-05 - mae: 0.0051 - val_loss: 2.5287e-05 - val_mae: 0.0037\n",
      "Model training finished for 15min with TSTEPS=5.\n",
      "Model saved to C:\\Users\\Anton\\SRC\\my\\ml_lstm\\models\\registry\\my_lstm_model_15min_tsteps5_20251126_163752.keras with validation loss 0.0000\n",
      "Calculating bias correction from validation set...\n",
      "\n",
      "\u001b[1m  1/218\u001b[0m \u001b[37m====================\u001b[0m \u001b[1m39s\u001b[0m 180ms/step\n",
      "\u001b[1m 36/218\u001b[0m \u001b[32m===\u001b[0m\u001b[37m=================\u001b[0m \u001b[1m0s\u001b[0m 1ms/step   \n",
      "\u001b[1m 77/218\u001b[0m \u001b[32m=======\u001b[0m\u001b[37m=============\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m119/218\u001b[0m \u001b[32m==========\u001b[0m\u001b[37m==========\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m164/218\u001b[0m \u001b[32m===============\u001b[0m\u001b[37m=====\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m209/218\u001b[0m \u001b[32m===================\u001b[0m\u001b[37m=\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m218/218\u001b[0m \u001b[32m====================\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Mean residual on validation set (log returns): -0.000121\n",
      "Bias correction (mean residual) saved to C:\\Users\\Anton\\SRC\\my\\ml_lstm\\models\\registry\\bias_correction_15min_tsteps5_20251126_163752.json\n",
      "Updated best hyperparameters for Frequency: 15min, TSTEPS: 5 with validation loss 0.0000\n",
      "\n",
      "Updated best hyperparameters saved to best_hyperparameters.json\n",
      "\n",
      "--- Training Summary ---\n",
      "Frequency: 15min, TSTEPS: 5, Validation Loss: 0.0000, Model: my_lstm_model_15min_tsteps5_20251126_163752.keras, Bias Correction: bias_correction_15min_tsteps5_20251126_163752.json\n",
      "\n",
      "2025-11-26 16:37:34.328193: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-26 16:37:37.158576: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-26 16:37:39.718177: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train an LSTM model for the chosen FREQUENCY and TSTEPS using src.train. \n",
    "run_module('src.train', '--frequency', FREQUENCY, '--tsteps', TSTEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a071051d",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 3. Evaluate latest best model\n",
    "Evaluate the latest best model (based on `best_hyperparameters.json`) using `src.evaluate_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e2c644ff",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: C:\\Users\\Anton\\SRC\\my\\ml_lstm\\venv\\Scripts\\python.exe -m src.evaluate_model in C:\\Users\\Anton\\SRC\\my\\ml_lstm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the latest best model using src.evaluate_model. \n",
    "run_module('src.evaluate_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f74ad6",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 4. Generate per-bar predictions CSV\n",
    "Use `scripts.generate_predictions_csv` to create a per-bar predictions file for NVDA at the chosen `FREQUENCY`.Output columns: `Time`, `predicted_price`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "870db720",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: C:\\Users\\Anton\\SRC\\my\\ml_lstm\\venv\\Scripts\\python.exe -m scripts.generate_predictions_csv --frequency 15min --output C:\\Users\\Anton\\SRC\\my\\ml_lstm\\backtests\\nvda_15min_predictions.csv in C:\\Users\\Anton\\SRC\\my\\ml_lstm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate per-bar predictions CSV for NVDA at the chosen FREQUENCY. \n",
    "run_module('scripts.generate_predictions_csv', '--frequency', FREQUENCY, '--output', PREDICTIONS_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cb6513",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 5. Run backtest using LSTM model\n",
    "Run `src.backtest` over processed OHLC data, using the LSTM model directly for predictions.\n",
    "\n",
    "- `--prediction-mode model` runs live model inference (log-return predictions converted to prices).\n",
    "- Exports trades and equity curve CSVs for diagnostics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f61bbb69",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: C:\\Users\\Anton\\SRC\\my\\ml_lstm\\venv\\Scripts\\python.exe -m src.backtest --frequency 15min --prediction-mode model --export-trades-csv C:\\Users\\Anton\\SRC\\my\\ml_lstm\\backtests\\nvda_15min_trades.csv --export-equity-csv C:\\Users\\Anton\\SRC\\my\\ml_lstm\\backtests\\nvda_15min_equity.csv --report\n",
      "in C:\\Users\\Anton\\SRC\\my\\ml_lstm\n",
      "\n",
      "Loaded 54070 model predictions from checkpoint at backtests\\nvda_15min_model_predictions_checkpoint.csv\n",
      "\n",
      "=======================================================\n",
      "  BACKTEST REPORT: NVDA 15min\n",
      "  2022-12-30 -> 2025-11-24\n",
      "=======================================================\n",
      "  Trades:                     254\n",
      "  Win Rate:                 22.4%\n",
      "  Total Return:           -100.0%\n",
      "  Max Drawdown:           -100.0%\n",
      "  Sharpe Ratio:             -1.23\n",
      "  Profit Factor:             0.34\n",
      "  CAGR:                      0.0%\n",
      "=======================================================\n",
      "  Equity:              10,000 -> -2\n",
      "  Plot: backtests\\NVDA-15min-0.25-0.70-0.0100-2.50-20221230-20251124.png\n",
      "=======================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run a backtest using the LSTM model directly for predictions.\n",
    "run_module('src.backtest', \n",
    "           '--frequency', FREQUENCY, \n",
    "           '--prediction-mode', 'model',\n",
    "           '--export-trades-csv', TRADES_CSV,\n",
    "           '--export-equity-csv', EQUITY_CSV,\n",
    "           '--report')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2fee6b",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 6. Plot backtest diagnostics\n",
    "Use `scripts.plot_backtest_diagnostics` to visualize equity curve and trade entry density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "609fe26b-bb3b-4923-9882-9dcbf005aab1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equity CSV: C:\\Users\\Anton\\SRC\\my\\ml_lstm\\backtests\\nvda_15min_equity.csv exists: True\n",
      "Trades CSV: C:\\Users\\Anton\\SRC\\my\\ml_lstm\\backtests\\nvda_15min_trades.csv exists: True\n",
      "Price CSV: C:\\Users\\Anton\\SRC\\my\\ml_lstm\\data\\processed\\nvda_15min.csv exists: True\n",
      "Saved diagnostics plot to C:\\Users\\Anton\\SRC\\my\\ml_lstm\\backtests\\backtest_diagnostics.png\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from scripts.plot_backtest_diagnostics import plot_equity_and_trade_density\n",
    "from src.config import (\n",
    "    FREQUENCY,\n",
    "    get_equity_csv_path,\n",
    "    get_trades_csv_path,\n",
    "    get_hourly_data_csv_path,\n",
    ")\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "\n",
    "equity_path = project_root / get_equity_csv_path(\"nvda\", FREQUENCY)\n",
    "trades_path = project_root / get_trades_csv_path(\"nvda\", FREQUENCY)\n",
    "price_path = project_root / get_hourly_data_csv_path(FREQUENCY)\n",
    "\n",
    "print(\"Equity CSV:\", equity_path, \"exists:\", equity_path.exists())\n",
    "print(\"Trades CSV:\", trades_path, \"exists:\", trades_path.exists())\n",
    "print(\"Price CSV:\", price_path, \"exists:\", price_path.exists())\n",
    "\n",
    "plot_equity_and_trade_density(\n",
    "    equity_path=str(equity_path),\n",
    "    trades_path=str(trades_path),\n",
    "    price_csv_path=str(price_path),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e1d162",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 7. Paper trading over historical data\n",
    "Use `src.paper_trade` to run a simulated paper-trading session over historical NVDA data, reusing the predictions CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707f8af2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run simulated paper trading over historical data using the predictions CSV.\n",
    "run_module('src.paper_trade', '--frequency', FREQUENCY, '--predictions-csv', PREDICTIONS_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1711db91",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# 8. (Optional) Hyperparameter tuning and experiment grid\n",
    "Not required for the minimal ingestion òÆÒ paper-trading loop, but useful for experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd8ba2-97dd-401b-90bd-b6a569695701",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning (can be long-running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81938906",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_module('src.tune_hyperparameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5eb61f-af82-4da4-9fe9-e8104e22ea4b",
   "metadata": {},
   "source": [
    "# Grid of experiments (very long-running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf52b1a-d33e-40e0-9820-5d8caeee869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_module('src.experiment_runner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d1b4d-d891-4b44-adf3-d30d37977f31",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Plot 15mins data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f84f4-1705-4034-86db-992186454745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# PRICE_CSV is already defined in the first cell of this notebook\n",
    "price_path = Path(PRICE_CSV)\n",
    "\n",
    "# Load the 15‑minute OHLC data\n",
    "prices = pd.read_csv(price_path)\n",
    "\n",
    "# Ensure we have a proper datetime column\n",
    "prices[\"Time\"] = pd.to_datetime(prices[\"Time\"])\n",
    "prices = prices.sort_values(\"Time\")\n",
    "\n",
    "# Choose which price to plot (use 'Close' if available)\n",
    "price_col = \"Close\" if \"Close\" in prices.columns else prices.columns[1]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(prices[\"Time\"], prices[price_col], label=f\"{price_col} price (15m)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"NVDA 15‑minute price over time\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
